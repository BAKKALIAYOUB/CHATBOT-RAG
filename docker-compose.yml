version: '3.8'

services:
  back-end:
    build:
      context: .
      dockerfile: Back-end(Flask)/Dockerfile
    ports:
      - "5000:5000"
    networks:
      - chromadb_network
    depends_on:
      - pull_model
      - ollama


  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    networks:
      - chromadb_network
    volumes:
      - ollama_model_data:/root/.ollama/models/
    deploy:
      resources:
        limits:
          cpus: '4.0'       # Limit the service to 4 CPUs
          memory: 8g        # Limit the service to 8 GB of memory
        reservations:
          cpus: '2.0'       # Reserve at least 2 CPUs
          memory: 4g        # Reserve at least 4 GB of memory
    
  pull_model:
    image: python:3.8-slim
    depends_on:
      - ollama
    networks:
      - chromadb_network
    command: >
      bash -c "apt-get update && apt-get install -y curl && curl -X POST -H 'Content-Type: application/json' -d '{\"name\":\"gemma:2b-instruct\"}' http://ollama:11434/api/pull"

  front_end:
    build:
      context: .
      dockerfile: Front-end(NextJS)/Dockerfile
    ports:
      - "3000:3000"
    depends_on:
      - back-end
    networks:
      - chromadb_network


networks:
  chromadb_network:
    driver: bridge

volumes:
  ollama_model_data:
    driver : local
